{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUm7DuHFQCBJUz/TKI0luB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadaatom/Comp472_Project/blob/main/MaskDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCfakfQPrwcQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip Images.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "id": "oO0itxEcsKwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING IMPORTS #\n",
        "import os\n",
        "import splitfolders\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "YppYMppNsMXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING IMPORTS #\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "OY9F4JTsuQmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION IMPORTS #\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "lfGnSiXjt8PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "================================ PREPROCESSING ================================"
      ],
      "metadata": {
        "id": "tz-I7GPRtvAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagePath = \"Images\"\n",
        "outputPath = \"train_test_sets\"\n",
        "\n",
        "trainDir = outputPath+\"/train\"\n",
        "testDir = outputPath+\"/test\"\n",
        "\n",
        "classes = [\"None\", \"N95\", \"Surgical\", \"Cloth\"] #Folders should be labeled the same as these classes."
      ],
      "metadata": {
        "id": "3ve5KGR_sOJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.ratio(imagePath, output=outputPath, seed=0, ratio=(.8, 0.1,.1))"
      ],
      "metadata": {
        "id": "cWuQWg9PsQoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 100\n",
        "img_width = 100"
      ],
      "metadata": {
        "id": "DxvAhHrrsSXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  trainDir,\n",
        "  seed=0,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  testDir,\n",
        "  seed=0,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vAk1TUFFsUef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "================================ TRAINING ================================"
      ],
      "metadata": {
        "id": "lmE2aCbDtqi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "f_dtvZU4siBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv_layer = nn.Sequential(\n",
        "\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(8 * 8 * 64, 1000),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1000, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(512, 10)\n",
        "    )"
      ],
      "metadata": {
        "id": "f7EoQQJSslnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    # conv layers\n",
        "    x = self.conv_layer(x)\n",
        "\n",
        "    # flatten\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # fc layer\n",
        "    x = self.fc_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Dht4XASrs9S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "J-Q_gwHxtRDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "        # Backprop and optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Train accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list.append(correct / total)\n",
        "        \n",
        "if (i + 1) % 100 == 0:\n",
        "    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ybduMYN2tQVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "============================= PREDICTION TIME BABY!! ============================="
      ],
      "metadata": {
        "id": "ahWU7zGgukRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UHh we gotta figure out this part but we need a y_true and y_pred\n",
        "y_true = []\n",
        "y_pred = []"
      ],
      "metadata": {
        "id": "KOv-B-5puj0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================== EVALUATION =================================="
      ],
      "metadata": {
        "id": "3u50TETTtb93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "HEJFsTxCtyjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "cm_plot_labels = [\"no mask\", \"cloth\", \"surgical\", \"n95\"]\n",
        "cm = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=cm_plot_labels)"
      ],
      "metadata": {
        "id": "KYcq_Ixeur9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=\"Confusion Matrix\", cmap=plt.cm.Reds)"
      ],
      "metadata": {
        "id": "y05Iu4o5uuX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_true, y_pred, average=\"macro\")\n",
        "recall = recall_score(y_true, y_pred, average=\"macro\")\n",
        "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"precision: {precision: .2f}\")\n",
        "print(f\"recall: {recall: .2f}\")\n",
        "print(f\"f1: {f1: .2f}\")\n",
        "print(f\"accuracy: {accuracy: .2f}\")"
      ],
      "metadata": {
        "id": "5bM_AVQauwSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}